import streamlit as st
import cv2
import numpy as np
import os
import time
import av
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration

# è¨­å®š TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from tensorflow.keras.models import load_model

# --- åƒæ•¸è¨­å®š ---
MIN_HEIGHT = 32
MIN_AREA = 140
SHRINK_PX = 4
STABILITY_DURATION = 1.2
MOVEMENT_THRESHOLD = 80

# --- 1. è¼‰å…¥æ¨¡å‹ (å…¨åŸŸå¿«å–) ---
@st.cache_resource
def load_ai_model():
    if os.path.exists("mnist_cnn.h5"):
        try:
            return load_model("mnist_cnn.h5")
        except:
            return None
    return None

model = load_ai_model()

# --- 2. å®šç¾©å½±åƒè™•ç†æ ¸å¿ƒ (é¡ä¼¼åŸæœ¬çš„ Class) ---
class HandwriteProcessor(VideoProcessorBase):
    def __init__(self):
        self.model = model
        self.last_boxes = []
        self.stability_start_time = None
        self.is_captured = False
        self.capture_cooldown = 0
        self.captured_frame = None
        
    # è†šè‰²éæ¿¾
    def is_valid_content(self, img_bgr):
        if img_bgr is None or img_bgr.size == 0: return False
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
        mean_h = np.mean(hsv[:,:,0])
        mean_s = np.mean(hsv[:,:,1])
        if mean_s > 60: return False
        if 30 < mean_s <= 60:
            if (mean_h < 25 or mean_h > 155): return False
        return True

    # ç©©å®šåº¦æª¢æŸ¥
    def check_stability(self, current_boxes):
        if len(current_boxes) == 0:
            self.stability_start_time = None
            return False, 0.0
        
        if len(self.last_boxes) == 0:
            self.last_boxes = current_boxes
            self.stability_start_time = time.time()
            return False, 0.0

        total_movement = 0
        for curr_box in current_boxes:
            c_x, c_y, c_w, c_h = curr_box["box"]
            min_dist = 99999
            for last_box in self.last_boxes:
                l_x, l_y, l_w, l_h = last_box["box"]
                dist = abs(c_x - l_x) + abs(c_y - l_y)
                if dist < min_dist: min_dist = dist
            
            if min_dist < 30: total_movement += min_dist
            else: total_movement += 20 

        count_diff = abs(len(current_boxes) - len(self.last_boxes))
        total_movement += count_diff * 30 
        self.last_boxes = current_boxes

        if total_movement < MOVEMENT_THRESHOLD:
            if self.stability_start_time is None:
                self.stability_start_time = time.time()
            elapsed = time.time() - self.stability_start_time
            progress = min(elapsed / STABILITY_DURATION, 1.0)
            return (elapsed >= STABILITY_DURATION), progress
        else:
            self.stability_start_time = time.time()
            return False, 0.0

    # æ¯ä¸€å€‹å½±æ ¼éƒ½æœƒè·‘é€²ä¾†é€™è£¡è™•ç†
    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        # å¦‚æœè™•æ–¼ã€Œå·²æŠ“æ‹å‡çµã€ç‹€æ…‹ï¼Œé¡¯ç¤ºå‡çµç•«é¢
        current_time = time.time()
        if self.is_captured:
            if current_time < self.capture_cooldown:
                # ä¿æŒé¡¯ç¤ºåŒä¸€å¼µåœ–ï¼Œä¸¦é¡¯ç¤ºå€’æ•¸
                display_img = self.captured_frame.copy()
                remaining = int(self.capture_cooldown - current_time) + 1
                cv2.putText(display_img, f"CAPTURED! Reset: {remaining}s", (20, 60), 
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
                return av.VideoFrame.from_ndarray(display_img, format="bgr24")
            else:
                # æ™‚é–“åˆ°ï¼Œè§£é–
                self.is_captured = False
                self.stability_start_time = None
        
        # --- Live åµæ¸¬æµç¨‹ ---
        display_img = img.copy()
        h_f, w_f = img.shape[:2]
        
        # 1. ç¹ªè£½è—è‰² ROI æ¡† (ä½ æœ€æƒ³è¦çš„ï¼)
        roi_rect = [10, 10, w_f - 20, h_f - 20]
        cv2.rectangle(display_img, (roi_rect[0], roi_rect[1]), 
                      (roi_rect[0]+roi_rect[2], roi_rect[1]+roi_rect[3]), (255, 0, 0), 2)
        
        # 2. å½±åƒå‰è™•ç†
        roi_img = img[roi_rect[1]:roi_rect[1]+roi_rect[3], roi_rect[0]:roi_rect[0]+roi_rect[2]]
        gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 45, 18)
        binary_proc = cv2.dilate(thresh, None, iterations=2)
        
        # 3. æ‰¾è¼ªå»“
        contours, hierarchy = cv2.findContours(binary_proc, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
        
        valid_boxes = []
        if hierarchy is not None:
            for i, cnt in enumerate(contours):
                if hierarchy[0][i][3] == -1:
                    area = cv2.contourArea(cnt)
                    if area > MIN_AREA:
                        x, y, w, h = cv2.boundingRect(cnt)
                        has_hole = hierarchy[0][i][2] != -1
                        valid_boxes.append({
                            "box": (x, y, w, h), 
                            "has_hole": has_hole,
                            "aspect_ratio": w / float(h)
                        })
        
        valid_boxes = sorted(valid_boxes, key=lambda b: b["box"][0])
        
        # 4. æ‰¹é‡é æ¸¬
        batch_rois = []
        batch_info = []
        raw_boxes_for_stability = [] # ç”¨ä¾†ç®—ç©©å®šåº¦çš„
        
        for item in valid_boxes:
            x, y, w, h = item["box"]
            rx, ry = x + roi_rect[0], y + roi_rect[1]
            
            # é‚Šç·£éæ¿¾
            if x < 15 or y < 15 or (x+w) > binary_proc.shape[1]-15 or (y+h) > binary_proc.shape[0]-15: continue
            if h < MIN_HEIGHT: continue
            
            # è†šè‰²éæ¿¾ (åœ¨åŸåœ–ä¸Šåˆ‡)
            roi_color = display_img[ry:ry+h, rx:rx+w]
            if not self.is_valid_content(roi_color): continue
            
            raw_boxes_for_stability.append(item)
            
            # CNN Padding
            roi_single = binary_proc[y:y+h, x:x+w]
            side = max(w, h)
            padding = int(side * 0.2)
            container_size = side + padding * 2
            container = np.zeros((container_size, container_size), dtype=np.uint8)
            offset_y = (container_size - h) // 2
            offset_x = (container_size - w) // 2
            container[offset_y:offset_y+h, offset_x:offset_x+w] = roi_single
            roi_resized = cv2.resize(container, (28, 28), interpolation=cv2.INTER_AREA)
            roi_norm = roi_resized.astype('float32') / 255.0
            roi_ready = roi_norm.reshape(28, 28, 1)
            
            batch_rois.append(roi_ready)
            batch_info.append({
                "coords": (rx, ry, w, h),
                "has_hole": item["has_hole"],
                "aspect": item["aspect_ratio"]
            })
            
        # 5. åŸ·è¡Œé æ¸¬èˆ‡ç¹ªåœ–
        detected_something = False
        if len(batch_rois) > 0 and self.model is not None:
            detected_something = True
            batch_input = np.stack(batch_rois)
            try:
                predictions = self.model.predict(batch_input, verbose=0)
                
                for i, pred in enumerate(predictions):
                    res_id = np.argmax(pred)
                    confidence = np.max(pred)
                    info = batch_info[i]
                    rx, ry, w, h = info["coords"]
                    has_hole = info["has_hole"]
                    aspect = info["aspect"]
                    
                    # æ··åˆä¿®æ­£
                    if res_id == 1:
                        if aspect > 0.45: res_id = 7
                    elif res_id == 7:
                        if aspect < 0.25: res_id = 1
                    if res_id == 7 and has_hole: res_id = 9
                    if res_id == 9 and not has_hole and confidence < 0.95: res_id = 7
                    if res_id == 0 and aspect < 0.5: res_id = 1
                    
                    # ç•«ç¶ æ¡† (å…§ç¸®)
                    draw_x = rx + SHRINK_PX
                    draw_y = ry + SHRINK_PX
                    draw_w = max(1, w - (SHRINK_PX * 2))
                    draw_h = max(1, h - (SHRINK_PX * 2))
                    
                    cv2.rectangle(display_img, (draw_x, draw_y), (draw_x+draw_w, draw_y+draw_h), (0, 255, 0), 2)
                    cv2.putText(display_img, str(res_id), (rx, ry-8), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
            except:
                pass # é¿å… TensorFlow åŸ·è¡Œç·’è¡çª

        # 6. ç©©å®šåº¦èˆ‡é€²åº¦æ¢
        is_stable, progress = self.check_stability(raw_boxes_for_stability)
        
        # ç•«é€²åº¦æ¢
        bar_w = int(600 * progress)
        color = (0, 255, 255) if progress < 1.0 else (0, 255, 0)
        # å›ºå®šåœ¨ç•«é¢ä¸‹æ–¹
        cv2.rectangle(display_img, (20, h_f - 40), (20 + bar_w, h_f - 25), color, -1)
        cv2.rectangle(display_img, (20, h_f - 40), (620, h_f - 25), (255, 255, 255), 2)
        
        # è§¸ç™¼æŠ“æ‹
        if is_stable and detected_something:
            self.is_captured = True
            self.capture_cooldown = time.time() + 3.0 # å‡çµ 3 ç§’
            self.captured_frame = display_img.copy() # å­˜ä¸‹é€™ä¸€ç¬é–“çš„ç•«é¢
            cv2.putText(display_img, "CAPTURED!", (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
            
        return av.VideoFrame.from_ndarray(display_img, format="bgr24")

# --- 3. ä»‹é¢éƒ¨åˆ† ---
st.set_page_config(page_title="æ‰‹å¯«è¾¨è­˜ (Liveç‰ˆ)", page_icon="ğŸ“¹", layout="wide")

st.title("ğŸ“¹ æ‰‹å¯«æ•¸å­—è¾¨è­˜ (å³æ™‚å½±åƒç‰ˆ)")
st.caption("ç¾åœ¨ç•«é¢æœƒå³æ™‚é¡¯ç¤ºè—æ¡†èˆ‡è¾¨è­˜çµæœï¼Œæ‰‹ç©©ä½å¾Œæœƒè‡ªå‹•å€’æ•¸æŠ“æ‹ï¼")

if model is None:
    st.error("âŒ æ‰¾ä¸åˆ° `mnist_cnn.h5`ï¼")
    st.stop()

# å•Ÿå‹• WebRTC ä¸²æµ
webrtc_ctx = webrtc_streamer(
    key="handwriting-cnn",
    video_processor_factory=HandwriteProcessor,
    media_stream_constraints={"video": True, "audio": False},
    rtc_configuration=RTCConfiguration(
        {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
    )
)

st.divider()
st.markdown("**æ“ä½œèªªæ˜:**")
st.markdown("1. é»æ“Š `START` é–‹å•Ÿæ”å½±æ©Ÿã€‚")
st.markdown("2. è—è‰²æ¡†æ¡†æœƒè‡ªå‹•å°æº–ç•«é¢ã€‚")
st.markdown("3. **å°‡æ•¸å­—å¡ç‰‡æ‹¿ç©©**ï¼Œä¸‹æ–¹é€²åº¦æ¢æœƒé–‹å§‹è·‘ã€‚")
st.markdown("4. é€²åº¦æ¢æ»¿äº†æœƒé¡¯ç¤º **CAPTURED** ä¸¦å‡çµ 3 ç§’æ–¹ä¾¿ä½ çœ‹çµæœã€‚")