import streamlit as st
import cv2
import numpy as np
import os
import time
import av
from PIL import Image
from streamlit_drawable_canvas import st_canvas
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode

# è¨­å®š TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from tensorflow.keras.models import load_model

# --- åƒæ•¸è¨­å®š ---
MIN_HEIGHT = 32
MIN_AREA = 140
SHRINK_PX = 4
STABILITY_DURATION = 1.2
MOVEMENT_THRESHOLD = 80

# [å·²æ”¹å›] èª¿æ•´è—æ¡†å¤§å° (æ”¹å›åŸæœ¬çš„ 60)
ROI_MARGIN_X = 60   # å·¦å³ç•™ç™½ 
ROI_MARGIN_Y = 60   # ä¸Šä¸‹ç•™ç™½

TEXT_Y_OFFSET = 15 
INFO_PANEL_WIDTH = 450 

# --- 1. è¼‰å…¥æ¨¡å‹ ---
@st.cache_resource
def load_ai_model():
    if os.path.exists("mnist_cnn.h5"):
        try:
            return load_model("mnist_cnn.h5")
        except:
            return None
    return None

model = load_ai_model()

# --- 2. æ ¸å¿ƒåŠŸèƒ½ ---
def is_valid_content(img_bgr):
    if img_bgr is None or img_bgr.size == 0: return False
    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
    mean_h = np.mean(hsv[:,:,0])
    mean_s = np.mean(hsv[:,:,1])
    if mean_s > 60: return False
    if 30 < mean_s <= 60:
        if (mean_h < 25 or mean_h > 155): return False
    return True

# æ‰‹å¯«æ¨¡å¼å°ˆç”¨ï¼šID è¿½è¹¤èˆ‡åŒ¹é…
def update_tracker(contours):
    current_items = []
    for cnt in contours:
        M = cv2.moments(cnt)
        if M["m00"] != 0:
            cx = int(M["m10"] / M["m00"])
            cy = int(M["m01"] / M["m00"])
            current_items.append({'cnt': cnt, 'center': (cx, cy), 'id': None})

    used_current_indices = set()
    new_tracker_state = {}
    
    if 'tracker_state' in st.session_state:
        for old_id, old_center in st.session_state['tracker_state'].items():
            min_dist = 9999
            match_idx = -1
            for i, item in enumerate(current_items):
                if i in used_current_indices: continue
                dist = np.hypot(item['center'][0]-old_center[0], item['center'][1]-old_center[1])
                if dist < 50 and dist < min_dist:
                    min_dist = dist
                    match_idx = i
            
            if match_idx != -1:
                current_items[match_idx]['id'] = old_id
                used_current_indices.add(match_idx)
                new_tracker_state[old_id] = current_items[match_idx]['center']

    for i, item in enumerate(current_items):
        if item['id'] is None:
            item['id'] = st.session_state['next_id']
            st.session_state['next_id'] += 1
            new_tracker_state[item['id']] = item['center']

    st.session_state['tracker_state'] = new_tracker_state
    current_items.sort(key=lambda x: x['id'])
    return current_items

# --- 3. WebRTC å½±åƒè™•ç†å™¨ ---
class HandwriteProcessor(VideoProcessorBase):
    def __init__(self):
        self.model = model
        self.last_boxes = []
        self.stability_start_time = None
        self.frozen = False        
        self.frozen_frame = None  
        self.detected_count = 0   
        self.ui_results = [] 

    def resume(self):
        self.frozen = False
        self.stability_start_time = None
        self.last_boxes = []
        self.ui_results = [] 

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        if self.frozen and self.frozen_frame is not None:
            return av.VideoFrame.from_ndarray(self.frozen_frame, format="bgr24")
        
        display_img = img.copy()
        h_f, w_f = img.shape[:2]
        
        # [ä¿®æ”¹] ä½¿ç”¨ X, Y Margin ä¾†è¨ˆç®— roi_rect
        roi_rect = [ROI_MARGIN_X, ROI_MARGIN_Y, w_f - 2*ROI_MARGIN_X, h_f - 2*ROI_MARGIN_Y]
        
        cv2.rectangle(display_img, (roi_rect[0], roi_rect[1]), 
                      (roi_rect[0]+roi_rect[2], roi_rect[1]+roi_rect[3]), (255, 0, 0), 2)
        
        roi_img = img[roi_rect[1]:roi_rect[1]+roi_rect[3], roi_rect[0]:roi_rect[0]+roi_rect[2]]
        
        if roi_img.size == 0:
             return av.VideoFrame.from_ndarray(display_img, format="bgr24")

        gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 45, 18)
        binary_proc = cv2.dilate(thresh, None, iterations=2)
        
        contours, hierarchy = cv2.findContours(binary_proc, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
        
        valid_boxes = []
        if hierarchy is not None:
            for i, cnt in enumerate(contours):
                if hierarchy[0][i][3] == -1:
                    area = cv2.contourArea(cnt)
                    if area > MIN_AREA:
                        x, y, w, h = cv2.boundingRect(cnt)
                        has_hole = hierarchy[0][i][2] != -1
                        valid_boxes.append({
                            "box": (x, y, w, h), 
                            "has_hole": has_hole,
                            "aspect_ratio": w / float(h)
                        })
        
        valid_boxes = sorted(valid_boxes, key=lambda b: b["box"][0])
        
        batch_rois = []
        batch_info = []
        raw_boxes_for_stability = [] 
        
        for item in valid_boxes:
            x, y, w, h = item["box"]
            rx, ry = x + roi_rect[0], y + roi_rect[1]
            
            if x < 5 or y < 5 or (x+w) > binary_proc.shape[1]-5 or (y+h) > binary_proc.shape[0]-5: continue
            if h < MIN_HEIGHT: continue
            
            roi_color = display_img[ry:ry+h, rx:rx+w]
            if not is_valid_content(roi_color): continue
            
            raw_boxes_for_stability.append(item)
            
            roi_single = binary_proc[y:y+h, x:x+w]
            side = max(w, h)
            padding = int(side * 0.2)
            container_size = side + padding * 2
            container = np.zeros((container_size, container_size), dtype=np.uint8)
            offset_y = (container_size - h) // 2
            offset_x = (container_size - w) // 2
            container[offset_y:offset_y+h, offset_x:offset_x+w] = roi_single
            roi_resized = cv2.resize(container, (28, 28), interpolation=cv2.INTER_AREA)
            roi_norm = roi_resized.astype('float32') / 255.0
            roi_ready = roi_norm.reshape(28, 28, 1)
            
            batch_rois.append(roi_ready)
            batch_info.append({
                "coords": (rx, ry, w, h),
                "has_hole": item["has_hole"],
                "aspect": item["aspect_ratio"]
            })
            
        detected_count = 0
        detected_something = False
        current_frame_text_results = []

        if len(batch_rois) > 0 and self.model is not None:
            detected_something = True
            try:
                batch_input = np.stack(batch_rois)
                predictions = self.model.predict(batch_input, verbose=0)
                
                for i, pred in enumerate(predictions):
                    top_indices = pred.argsort()[-3:][::-1]
                    res_id = top_indices[0]
                    confidence = pred[res_id]
                    
                    info = batch_info[i]
                    rx, ry, w, h = info["coords"]
                    has_hole = info["has_hole"]
                    aspect = info["aspect"]
                    
                    if res_id == 1:
                        if aspect > 0.45: res_id = 7
                    elif res_id == 7:
                        if aspect < 0.25: res_id = 1
                    if res_id == 7 and has_hole: res_id = 9
                    if res_id == 9 and not has_hole and confidence < 0.95: res_id = 7
                    if res_id == 0 and aspect < 0.5: res_id = 1
                    
                    draw_x = rx + SHRINK_PX
                    draw_y = ry + SHRINK_PX
                    draw_w = max(1, w - (SHRINK_PX * 2))
                    draw_h = max(1, h - (SHRINK_PX * 2))
                    
                    cv2.rectangle(display_img, (draw_x, draw_y), (draw_x+draw_w, draw_y+draw_h), (0, 255, 0), 2)
                    
                    cv2.putText(display_img, f"#{i+1}", (rx, ry-10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    info_text = f"**#{i+1}**: æ•¸å­— `{res_id}` (ä¿¡å¿ƒåº¦: {int(confidence*100)}%)"
                    
                    if confidence < 1.0:
                        alt_id = top_indices[1]
                        alt_conf = pred[alt_id]
                        if alt_conf > 0.01:
                            info_text += f" âš ï¸ å…¶ä»–: `{alt_id}` ({int(alt_conf*100)}%)"
                            
                    current_frame_text_results.append(info_text)
                    detected_count += 1
            except: pass

        self.detected_count = detected_count

        if len(raw_boxes_for_stability) == 0:
            self.stability_start_time = None
        elif len(self.last_boxes) == 0:
            self.last_boxes = raw_boxes_for_stability
            self.stability_start_time = time.time()
        else:
            total_movement = 0
            for curr_box in raw_boxes_for_stability:
                c_x, c_y, c_w, c_h = curr_box["box"]
                min_dist = 99999
                for last_box in self.last_boxes:
                    l_x, l_y, l_w, l_h = last_box["box"]
                    dist = abs(c_x - l_x) + abs(c_y - l_y)
                    if dist < min_dist: min_dist = dist
                if min_dist < 30: total_movement += min_dist
                else: total_movement += 20 
            
            count_diff = abs(len(raw_boxes_for_stability) - len(self.last_boxes))
            total_movement += count_diff * 30 
            self.last_boxes = raw_boxes_for_stability

            if total_movement < MOVEMENT_THRESHOLD:
                if self.stability_start_time is None: self.stability_start_time = time.time()
                elapsed = time.time() - self.stability_start_time
                progress = min(elapsed / STABILITY_DURATION, 1.0)
                
                bar_y = h_f - 20 
                bar_w = int(600 * progress)
                color = (0, 255, 255) if progress < 1.0 else (0, 255, 0)
                
                cv2.rectangle(display_img, (20, bar_y - 15), (20 + bar_w, bar_y), color, -1)
                cv2.rectangle(display_img, (20, bar_y - 15), (w_f - 20, bar_y), (255, 255, 255), 2)
                
                if elapsed >= STABILITY_DURATION and detected_something:
                    self.frozen = True
                    # [ä¿®æ”¹] æ–‡å­—ä½ç½®æ ¹æ“š Y Margin èª¿æ•´
                    text_y = max(30, ROI_MARGIN_Y - TEXT_Y_OFFSET) 
                    cv2.putText(display_img, "CAPTURED!", (ROI_MARGIN_X, text_y), 
                                cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 2)
                    
                    self.frozen_frame = display_img.copy()
                    self.ui_results = current_frame_text_results
            else:
                self.stability_start_time = time.time()

        return av.VideoFrame.from_ndarray(display_img, format="bgr24")

# --- 4. Streamlit ä»‹é¢ ---
st.set_page_config(page_title="æ‰‹å¯«è¾¨è­˜ (Web çµ‚æ¥µç‰ˆ)", page_icon="ğŸ“", layout="wide")

if 'stats' not in st.session_state:
    st.session_state['stats'] = {'camera': {'total': 0, 'correct': 0}, 'handwriting': {'total': 0, 'correct': 0}}
if 'history' not in st.session_state:
    st.session_state['history'] = {'camera': [], 'handwriting': []}
if 'input_key' not in st.session_state:
    st.session_state['input_key'] = 0
if 'canvas_key' not in st.session_state:
    st.session_state['canvas_key'] = "canvas_0"
if 'tracker_state' not in st.session_state:
    st.session_state['tracker_state'] = {}
if 'next_id' not in st.session_state:
    st.session_state['next_id'] = 1
    
# [æ–°å¢] æ‰‹å¯«æ¨¡å¼çš„çµæœè¨˜æ†¶é«” (é˜²æ­¢è·³æ‰)
if 'hw_display_list' not in st.session_state:
    st.session_state['hw_display_list'] = []
if 'hw_result_img' not in st.session_state:
    st.session_state['hw_result_img'] = None
if 'hw_result_count' not in st.session_state:
    st.session_state['hw_result_count'] = 0

with st.sidebar:
    st.title("ğŸ›ï¸ æ§åˆ¶å°")
    app_mode = st.radio("æ¨¡å¼é¸æ“‡", ["ğŸ“· æ”å½±æ©Ÿæ¨¡å¼ (Live)", "ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼"])
    st.divider()
    
    st.markdown("### ğŸ“· é¡é ­æˆç¸¾")
    c_total = st.session_state['stats']['camera']['total']
    c_correct = st.session_state['stats']['camera']['correct']
    c_acc = (c_correct / c_total * 100) if c_total > 0 else 0.0
    col_c1, col_c2 = st.columns(2)
    with col_c1: st.metric("ç¸½æ•¸", c_total)
    with col_c2: st.metric("æ­£ç¢º", c_correct)
    st.metric("é¡é ­æº–ç¢ºç‡", f"{c_acc:.1f}%")
    
    col_undo_c, col_reset_c = st.columns(2)
    with col_undo_c:
        if st.button("â†©ï¸ å¾©åŸ", key="undo_cam"):
            if st.session_state['history']['camera']:
                last_entry = st.session_state['history']['camera'].pop()
                st.session_state['stats']['camera']['total'] -= last_entry['total']
                st.session_state['stats']['camera']['correct'] -= last_entry['correct']
                st.rerun()
            else:
                st.toast("âš ï¸ ç„¡ç´€éŒ„å¯å¾©åŸ")
    with col_reset_c:
        if st.button("ğŸ—‘ï¸ é‡ç½®", key="reset_cam"):
            st.session_state['stats']['camera'] = {'total': 0, 'correct': 0}
            st.session_state['history']['camera'] = []
            st.rerun()

    st.divider()

    st.markdown("### ğŸ¨ æ‰‹å¯«æˆç¸¾")
    h_total = st.session_state['stats']['handwriting']['total']
    h_correct = st.session_state['stats']['handwriting']['correct']
    h_acc = (h_correct / h_total * 100) if h_total > 0 else 0.0
    col_h1, col_h2 = st.columns(2)
    with col_h1: st.metric("ç¸½æ•¸", h_total)
    with col_h2: st.metric("æ­£ç¢º", h_correct)
    st.metric("æ‰‹å¯«æº–ç¢ºç‡", f"{h_acc:.1f}%")

    col_undo_h, col_reset_h = st.columns(2)
    with col_undo_h:
        if st.button("â†©ï¸ å¾©åŸ", key="undo_hw"):
            if st.session_state['history']['handwriting']:
                last_entry = st.session_state['history']['handwriting'].pop()
                st.session_state['stats']['handwriting']['total'] -= last_entry['total']
                st.session_state['stats']['handwriting']['correct'] -= last_entry['correct']
                st.rerun()
            else:
                st.toast("âš ï¸ ç„¡ç´€éŒ„å¯å¾©åŸ")
    with col_reset_h:
        if st.button("ğŸ—‘ï¸ é‡ç½®", key="reset_hw"):
            # ä¿®æ”¹è™•ï¼šåªæ¸…é™¤çµ±è¨ˆèˆ‡è¿½è¹¤å™¨ï¼Œä¿ç•™é¡¯ç¤ºçµæœï¼Œé¿å…ç•«é¢é–ƒçˆè®Šç©º
            st.session_state['stats']['handwriting'] = {'total': 0, 'correct': 0}
            st.session_state['history']['handwriting'] = []
            st.session_state['tracker_state'] = {}
            st.session_state['next_id'] = 1
            # st.session_state['hw_display_list'] = []  <-- ç§»é™¤é€™è¡Œ
            # st.session_state['hw_result_img'] = None  <-- ç§»é™¤é€™è¡Œ
            # st.session_state['hw_result_count'] = 0   <-- ç§»é™¤é€™è¡Œ
            st.rerun()

st.title("ğŸ“ æ‰‹å¯«æ•¸å­—è¾¨è­˜ç³»çµ±")

with st.expander("ğŸ“– ç³»çµ±æ“ä½œèªªæ˜ (é»æ“Šå±•é–‹)", expanded=False):
    st.markdown("""
    #### 1. ğŸ“· æ”å½±æ©Ÿæ¨¡å¼ (Live)
    * é»æ“Šä¸‹æ–¹çš„ **START** æŒ‰éˆ•é–‹å•Ÿæ”å½±æ©Ÿã€‚
    * å°‡æ‰‹å¯«æ•¸å­—ç´™å¼µç½®æ–¼è—è‰²æ¡†æ¡†å…§ã€‚
    * **ä¿æŒæ‰‹éƒ¨ç©©å®š**ï¼Œä¸‹æ–¹é€²åº¦æ¢æœƒé–‹å§‹è·‘ï¼Œè·‘æ»¿å¾Œç•«é¢é¡¯ç¤º **"CAPTURED"** ä¸¦å‡çµã€‚
    * ç•«é¢å‡çµå¾Œï¼Œ**é»æ“Šå³å´çš„ã€ŒğŸ“‹ é¡¯ç¤ºè¾¨è­˜è©³æƒ…ã€æŒ‰éˆ•**ã€‚
    * ç¢ºèªå¾Œï¼Œé»æ“Š **"ğŸ’¾ å„²å­˜ä¸¦ç¹¼çºŒ"**ã€‚
    * è‹¥æŠ“æ‹çµæœä¸ç†æƒ³ï¼Œå¯é»æ“Š **"ğŸ”„ é‡æ–°æ”å½±"**ã€‚

    #### 2. ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼
    * åˆ‡æ›è‡³å·¦å´é¸å–®çš„ã€Œæ‰‹å¯«æ¿æ¨¡å¼ã€ã€‚
    * ç›´æ¥åœ¨é»‘è‰²ç•«å¸ƒä¸Šç”¨æ»‘é¼ æ›¸å¯«æ•¸å­—ã€‚
    * æ”¾é–‹æ»‘é¼ å¾Œï¼Œ**å·¦å´ä¸‹æ–¹**é¡¯ç¤ºè©³ç´°ä¿¡å¿ƒåº¦ï¼Œ**å³å´**é¡¯ç¤ºç¶ æ¡†çµæœã€‚
    """)

if model is None:
    st.error("âŒ æ‰¾ä¸åˆ° `mnist_cnn.h5`ï¼")
    st.stop()

# --- 5. æ¨¡å¼åˆ†æ”¯ ---

if app_mode == "ğŸ“· æ”å½±æ©Ÿæ¨¡å¼ (Live)":
    
    col_cam, col_data = st.columns([2, 1])

    with col_cam:
        ctx = webrtc_streamer(
            key="handwrite-live",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=HandwriteProcessor,
            media_stream_constraints={"video": {"width": 1280, "height": 720}, "audio": False},
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            async_processing=True,
        )

    with col_data:
        st.markdown("### ğŸ“Š è©³ç´°æ•¸æ“š")
        st.caption("è«‹ç­‰å¾…ç•«é¢å‡ºç¾ Captured å¾Œï¼ŒæŒ‰ä¸‹æ–¹æŒ‰éˆ•æ›´æ–°æ•¸æ“š")
        
        # [ä¿®æ”¹] ä½¿ç”¨å…©æ¬„ä¾†æ”¾ç½®ã€Œé¡¯ç¤ºè©³æƒ…ã€å’Œã€Œé‡æ–°æ”å½±ã€
        col_btn1, col_btn2 = st.columns(2)
        
        with col_btn1:
            if st.button("ğŸ“‹ é¡¯ç¤ºè©³æƒ… (Update)", type="secondary", use_container_width=True):
                if ctx.video_processor and ctx.video_processor.frozen:
                    results = ctx.video_processor.ui_results
                    if results:
                        st.success(f"å…±åµæ¸¬åˆ° {len(results)} å€‹æ•¸å­—")
                        st.session_state['last_cam_detected'] = len(results)
                        for line in results:
                            st.markdown(line)
                    else:
                        st.warning("âš ï¸ ç•«é¢å‡çµäº†ï¼Œä½†æ²’æœ‰åµæ¸¬åˆ°æ•¸å­—ã€‚")
                        st.session_state['last_cam_detected'] = 0
                else:
                    st.info("â³ è«‹å…ˆç­‰å¾…é¡é ­ç•«é¢æŠ“æ‹å‡çµ (Captured)...")

        # [æ–°å¢] é‡æ–°æ”å½±æŒ‰éˆ•
        with col_btn2:
            if st.button("ğŸ”„ é‡æ–°æ”å½± (Retake)", type="primary", use_container_width=True):
                if ctx.video_processor:
                    ctx.video_processor.resume()
                # é‡æ–°åˆ·æ–°é é¢ä»¥æ›´æ–° UI ç‹€æ…‹
                st.session_state['last_cam_detected'] = 0
                st.rerun()

        st.write("---")
        
        manual_score = st.number_input("âœï¸ è¼¸å…¥æ­£ç¢ºæ•¸é‡", min_value=0, value=0, key=f"score_input_{st.session_state['input_key']}")
        
        st.write("##") 
        if st.button("ğŸ’¾ å„²å­˜ä¸¦ç¹¼çºŒ (Save & Resume)", type="primary", use_container_width=True):
            
            total_add = st.session_state.get('last_cam_detected', 0)
            if total_add > 0 and manual_score > total_add:
                st.error(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æ•¸å€¼ ({manual_score}) è¶…éåµæ¸¬ç¸½æ•¸ ({total_add})ï¼Œå¤šäº† {manual_score - total_add} å€‹ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼")
            else:
                if ctx.video_processor:
                    ctx.video_processor.resume()
                
                if total_add == 0: total_add = manual_score

                if manual_score > 0:
                    st.session_state['stats']['camera']['total'] += total_add
                    st.session_state['stats']['camera']['correct'] += manual_score
                    st.session_state['history']['camera'].append({
                        'total': total_add,
                        'correct': manual_score
                    })
                    st.toast(f"âœ… é¡é ­æ¨¡å¼ï¼šå·²è¨˜éŒ„ (ç¸½æ•¸{total_add}/æ­£ç¢º{manual_score})")
                    time.sleep(0.5)
                    st.session_state['input_key'] += 1
                    
                st.rerun()

elif app_mode == "ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼":
    
    c_left, c_right = st.columns([3, 1])

    current_results_list = []
    
    with c_left:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤ç•«å¸ƒ"):
            st.session_state['canvas_key'] = f"canvas_{time.time()}"
            st.session_state['tracker_state'] = {}
            st.session_state['next_id'] = 1
            # ä¿®æ”¹è™•ï¼šæ¸…é™¤ç•«å¸ƒæ™‚ï¼Œä¿ç•™ä¸Šä¸€æ¬¡çš„è¾¨è­˜çµæœåœ¨ç•«é¢ä¸Šï¼Œç›´åˆ°å¯«ä¸‹æ–°å­—
            # st.session_state['hw_display_list'] = []  <-- ç§»é™¤
            # st.session_state['hw_result_img'] = None  <-- ç§»é™¤
            # st.session_state['hw_result_count'] = 0   <-- ç§»é™¤
            st.rerun()

        canvas_result = st_canvas(
            fill_color="rgba(255, 165, 0, 0.3)",
            stroke_width=15,
            stroke_color="#FFFFFF",
            background_color="#000000",
            height=400,  
            width=850,   
            drawing_mode="freedraw",
            key=st.session_state['canvas_key'],
        )
        
        # è™•ç†é‚è¼¯
        if canvas_result.image_data is not None:
            img_data = canvas_result.image_data.astype(np.uint8)
            
            if np.max(img_data) > 0:
                if img_data.shape[2] == 4:
                    img_data = cv2.cvtColor(img_data, cv2.COLOR_RGBA2BGR)
                gray = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)
                binary_proc = cv2.dilate(gray, None, iterations=1)
                _, binary_proc = cv2.threshold(binary_proc, 127, 255, cv2.THRESH_BINARY)
                contours, _ = cv2.findContours(binary_proc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                tracked_items = update_tracker(contours)
                
                draw_img = img_data.copy()
                batch_rois = []
                
                for item in tracked_items:
                    cnt = item['cnt']
                    x, y, w, h = cv2.boundingRect(cnt)
                    
                    if cv2.contourArea(cnt) > MIN_AREA:
                        roi = binary_proc[y:y+h, x:x+w]
                        side = max(w, h)
                        pad = int(side * 0.2)
                        container = np.zeros((side+pad*2, side+pad*2), dtype=np.uint8)
                        ox, oy = (side+pad*2-w)//2, (side+pad*2-h)//2
                        container[oy:oy+h, ox:ox+w] = roi
                        roi_ready = cv2.resize(container, (28, 28), interpolation=cv2.INTER_AREA)
                        roi_ready = roi_ready.astype('float32') / 255.0
                        roi_ready = roi_ready.reshape(28, 28, 1)
                        batch_rois.append(roi_ready)
                
                detected_count = 0
                if len(batch_rois) > 0:
                    preds = model.predict(np.stack(batch_rois), verbose=0)
                    for i, pred in enumerate(preds):
                        item = tracked_items[i]
                        display_id = item['id']
                        cnt = item['cnt']
                        
                        top_indices = pred.argsort()[-3:][::-1]
                        res_id = top_indices[0]
                        confidence = pred[res_id]
                        
                        x, y, w, h = cv2.boundingRect(cnt)
                        asp = w/h
                        if res_id==1 and asp>0.5: res_id=7
                        if res_id==7 and asp<0.3: res_id=1
                        
                        cv2.rectangle(draw_img, (x, y), (x+w, y+h), (0, 255, 0), 2)
                        
                        # [ä¿®æ”¹] é¡¯ç¤ºè¾¨è­˜åˆ°çš„æ•¸å­— (res_id)ï¼Œå­—é«”ç¨å¾®æ”¾å¤§ (1.0)
                        cv2.putText(draw_img, str(res_id), (x, y-10), 
                                    cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                        
                        display_text = f"**#{display_id}**: æ•¸å­— `{res_id}` (ä¿¡å¿ƒåº¦: {int(confidence*100)}%)"
                        if confidence < 1.0: 
                            alts = []
                            for alt_idx in top_indices[1:]:
                                alt_conf = pred[alt_idx]
                                if alt_conf > 0.01:
                                    alts.append(f"`{alt_idx}` ({int(alt_conf*100)}%)")
                            if alts:
                                display_text += f" âš ï¸ å…¶ä»–: {', '.join(alts)}"
                        
                        current_results_list.append(display_text)
                        detected_count += 1
                
                # [é—œéµ] æ›´æ–°å…¨åŸŸè¨˜æ†¶é«” (åœ–ç‰‡ & æ•¸æ“š)
                if current_results_list:
                    st.session_state['hw_display_list'] = current_results_list
                    st.session_state['hw_result_img'] = draw_img
                    st.session_state['hw_result_count'] = detected_count
            else:
                # ç•«å¸ƒè®Šç©ºæ™‚ï¼Œä¸æ¸…ç©ºè¨˜æ†¶ï¼Œä¿ç•™ä¸Šä¸€æ¬¡çµæœï¼Œç›´åˆ°æŒ‰æ¸…é™¤
                pass

    with c_left:
        # å„ªå…ˆé¡¯ç¤ºè¨˜æ†¶é«”ä¸­çš„è³‡æ–™
        if st.session_state['hw_display_list']:
            st.write("---")
            st.markdown("#### ğŸ“Š è©³ç´°æ•¸æ“š:")
            cols = st.columns(2)
            for i, text in enumerate(st.session_state['hw_display_list']):
                cols[i % 2].markdown(text)

    with c_right:
        st.markdown("### ğŸ‘ï¸ çµæœ")
        
        # å„ªå…ˆé¡¯ç¤ºè¨˜æ†¶é«”ä¸­çš„åœ–ç‰‡
        if st.session_state['hw_result_img'] is not None:
            st.image(st.session_state['hw_result_img'], channels="BGR", use_container_width=True)
        else:
            st.info("è«‹åœ¨å·¦å´æ›¸å¯«")

        st.write("---")
        
        # æ•¸é‡ä¾æ“šè¨˜æ†¶é«”
        final_count = st.session_state['hw_result_count']

        if final_count > 0:
            st.success(f"åµæ¸¬åˆ°: {final_count} å€‹")
        
        hw_score = st.number_input("è¼¸å…¥æ•¸é‡", min_value=0, value=final_count, key="hw_input")
        
        st.write("##")
        if st.button("ğŸ’¾ å„²å­˜", key="hw_save", type="primary"):
            
            # æ‰‹å¯«æ¨¡å¼é˜²å‘†
            if hw_score > final_count:
                st.error(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æ•¸å€¼ ({hw_score}) è¶…éåµæ¸¬ç¸½æ•¸ ({final_count})ï¼Œå¤šäº† {hw_score - final_count} å€‹ï¼Œè«‹é‡æ–°è¼¸å…¥ï¼")
            else:
                st.session_state['stats']['handwriting']['total'] += final_count
                st.session_state['stats']['handwriting']['correct'] += hw_score
                
                st.session_state['history']['handwriting'].append({
                    'total': final_count,
                    'correct': hw_score
                })
                
                # [æ–°å¢] å„²å­˜å¾Œï¼Œæ¸…ç©ºç•«å¸ƒèˆ‡çµæœ
                st.session_state['canvas_key'] = f"canvas_{time.time()}"
                st.session_state['tracker_state'] = {}
                st.session_state['next_id'] = 1
                st.session_state['hw_display_list'] = []
                st.session_state['hw_result_img'] = None
                st.session_state['hw_result_count'] = 0
                
                st.toast("âœ… æ‰‹å¯«æˆç¸¾å·²å„²å­˜ï¼")
                time.sleep(0.5)
                st.rerun()