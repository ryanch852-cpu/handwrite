import streamlit as st
import cv2
import numpy as np
import os
import time
import av
import joblib  # ç”¨æ–¼å„²å­˜/è®€å– KNN æ¨¡å‹
from PIL import Image
from streamlit_drawable_canvas import st_canvas
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode

# è¨­å®š TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets import mnist  # ç”¨æ–¼è¨“ç·´ KNN
from sklearn.neighbors import KNeighborsClassifier

# --- åƒæ•¸è¨­å®š ---
# [è·é›¢æ§åˆ¶]
MIN_HEIGHT = 50       
MIN_AREA = 500       

SHRINK_PX = 4
STABILITY_DURATION = 1.2
MOVEMENT_THRESHOLD = 80

# [éæ¿¾] ç¬¬ä¸€é“é˜²ç·šï¼šCNN ä¿¡å¿ƒåº¦é–€æª»
CONFIDENCE_THRESHOLD = 0.85 

# [é›™é‡é©—è­‰] ç¬¬äºŒé“é˜²ç·šï¼šç°è‰²åœ°å¸¶
KNN_VERIFY_RANGE = (0.85, 0.95)

# [è¨­å®š] è—æ¡†å¤§å°
ROI_MARGIN_X = 60   
ROI_MARGIN_Y = 60   
TEXT_Y_OFFSET = 15 

# --- 1. æ¨¡å‹è¼‰å…¥èˆ‡åˆå§‹åŒ– ---

@st.cache_resource
def load_ai_models():
    # 1. è¼‰å…¥ CNN
    cnn = None
    if os.path.exists("mnist_cnn.h5"):
        try:
            cnn = load_model("mnist_cnn.h5")
            print("âœ… CNN æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except:
            print("âŒ CNN æ¨¡å‹è¼‰å…¥å¤±æ•—")
    
    # 2. è¼‰å…¥æˆ–è¨“ç·´ KNN (ä½œç‚ºç¬¬äºŒé“é˜²ç·š)
    knn = None
    knn_path = "knn_model.pkl"
    
    if os.path.exists(knn_path):
        try:
            knn = joblib.load(knn_path)
            print("âœ… KNN æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        except:
            print("âš ï¸ KNN æ¨¡å‹æå£ï¼Œé‡æ–°è¨“ç·´...")
    
    # å¦‚æœæ²’æœ‰ KNN æ¨¡å‹ï¼Œç¾å ´è¨“ç·´ä¸€å€‹ (è¼•é‡ç‰ˆ)
    if knn is None:
        print("â³ æ­£åœ¨è¨“ç·´ KNN è¼”åŠ©æ¨¡å‹ (åƒ…éœ€ä¸€æ¬¡)...")
        try:
            (x_train, y_train), _ = mnist.load_data()
            x_flat = x_train.reshape(-1, 784) / 255.0
            
            # ç‚ºäº†å•Ÿå‹•é€Ÿåº¦ï¼Œåªç”¨å‰ 10000 ç­†è³‡æ–™è¨“ç·´
            knn = KNeighborsClassifier(n_neighbors=3)
            knn.fit(x_flat[:10000], y_train[:10000])
            
            joblib.dump(knn, knn_path)
            print("âœ… KNN æ¨¡å‹è¨“ç·´å®Œæˆä¸¦å„²å­˜")
        except Exception as e:
            print(f"âŒ KNN è¨“ç·´å¤±æ•—: {e}")
            knn = None

    return cnn, knn

model, knn_model = load_ai_models()

# --- [è‡ªå‹•æ‰¶æ­£] Deskewing ---
def deskew(img):
    m = cv2.moments(img)
    if abs(m['mu02']) < 1e-2:
        return img
    skew = m['mu11'] / m['mu02']
    M = np.float32([[1, skew, -0.5 * img.shape[0] * skew], [0, 1, 0]])
    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]), flags=cv2.WARP_INVERSE_MAP | cv2.INTER_LINEAR)
    return img

# --- 2. æ ¸å¿ƒæª¢æ¸¬åŠŸèƒ½ ---
def is_valid_content(img_bgr):
    if img_bgr is None or img_bgr.size == 0: return False
    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
    mean_h = np.mean(hsv[:,:,0])
    mean_s = np.mean(hsv[:,:,1])
    if mean_s > 60: return False
    if 30 < mean_s <= 60:
        if (mean_h < 25 or mean_h > 155): return False
    return True

# æ‰‹å¯«æ¨¡å¼å°ˆç”¨ï¼šID è¿½è¹¤èˆ‡åŒ¹é…
def update_tracker(contours):
    current_items = []
    for cnt in contours:
        M = cv2.moments(cnt)
        if M["m00"] != 0:
            cx = int(M["m10"] / M["m00"])
            cy = int(M["m01"] / M["m00"])
            current_items.append({'cnt': cnt, 'center': (cx, cy), 'id': None})

    used_current_indices = set()
    new_tracker_state = {}
    
    if 'tracker_state' in st.session_state:
        for old_id, old_center in st.session_state['tracker_state'].items():
            min_dist = 9999
            match_idx = -1
            for i, item in enumerate(current_items):
                if i in used_current_indices: continue
                dist = np.hypot(item['center'][0]-old_center[0], item['center'][1]-old_center[1])
                if dist < 50 and dist < min_dist:
                    min_dist = dist
                    match_idx = i
            
            if match_idx != -1:
                current_items[match_idx]['id'] = old_id
                used_current_indices.add(match_idx)
                new_tracker_state[old_id] = current_items[match_idx]['center']

    for i, item in enumerate(current_items):
        if item['id'] is None:
            item['id'] = st.session_state['next_id']
            st.session_state['next_id'] += 1
            new_tracker_state[item['id']] = item['center']

    st.session_state['tracker_state'] = new_tracker_state
    current_items.sort(key=lambda x: x['id'])
    return current_items

# --- 3. WebRTC å½±åƒè™•ç†å™¨ ---
class HandwriteProcessor(VideoProcessorBase):
    def __init__(self):
        self.model = model
        self.knn = knn_model
        self.last_boxes = []
        self.stability_start_time = None
        self.frozen = False        
        self.frozen_frame = None  
        self.detected_count = 0   
        self.ui_results = [] 
        
        self.frame_counter = 0
        self.skip_rate = 4  
        self.cached_rois = [] 

    def resume(self):
        self.frozen = False
        self.stability_start_time = None
        self.last_boxes = []
        self.ui_results = [] 
        self.frame_counter = 0

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        
        if self.frozen and self.frozen_frame is not None:
            return av.VideoFrame.from_ndarray(self.frozen_frame, format="bgr24")
        
        display_img = img.copy()
        h_f, w_f = img.shape[:2]
        
        roi_rect = [ROI_MARGIN_X, ROI_MARGIN_Y, w_f - 2*ROI_MARGIN_X, h_f - 2*ROI_MARGIN_Y]
        cv2.rectangle(display_img, (roi_rect[0], roi_rect[1]), 
                      (roi_rect[0]+roi_rect[2], roi_rect[1]+roi_rect[3]), (255, 0, 0), 2)

        self.frame_counter += 1
        process_this_frame = (self.frame_counter % self.skip_rate == 0)

        if not process_this_frame and len(self.cached_rois) > 0:
            for (dx, dy, dw, dh, txt, box_color) in self.cached_rois:
                cv2.rectangle(display_img, (dx, dy), (dx+dw, dy+dh), box_color, 2)
                cv2.putText(display_img, txt, (dx, dy-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            return av.VideoFrame.from_ndarray(display_img, format="bgr24")
        
        # --- è™•ç†é‚è¼¯ ---
        roi_img = img[roi_rect[1]:roi_rect[1]+roi_rect[3], roi_rect[0]:roi_rect[0]+roi_rect[2]]
        if roi_img.size == 0: return av.VideoFrame.from_ndarray(display_img, format="bgr24")

        gray = cv2.cvtColor(roi_img, cv2.COLOR_BGR2GRAY)
        blur = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 45, 18)
        binary_proc = cv2.dilate(thresh, None, iterations=2)
        
        contours, hierarchy = cv2.findContours(binary_proc, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
        
        valid_boxes = []
        if hierarchy is not None:
            for i, cnt in enumerate(contours):
                if hierarchy[0][i][3] == -1:
                    area = cv2.contourArea(cnt)
                    if area > MIN_AREA:
                        x, y, w, h = cv2.boundingRect(cnt)
                        has_hole = hierarchy[0][i][2] != -1
                        valid_boxes.append({
                            "box": (x, y, w, h), 
                            "has_hole": has_hole,
                            "aspect_ratio": w / float(h)
                        })
        
        valid_boxes = sorted(valid_boxes, key=lambda b: b["box"][0])
        
        batch_rois = []
        batch_info = []
        raw_boxes_for_stability = [] 
        
        self.cached_rois = []

        for item in valid_boxes:
            x, y, w, h = item["box"]
            rx, ry = x + roi_rect[0], y + roi_rect[1]
            
            if x < 5 or y < 5 or (x+w) > binary_proc.shape[1]-5 or (y+h) > binary_proc.shape[0]-5: continue
            if h < MIN_HEIGHT: continue
            
            roi_color = display_img[ry:ry+h, rx:rx+w]
            if not is_valid_content(roi_color): continue
            
            raw_boxes_for_stability.append(item)
            
            roi_single = binary_proc[y:y+h, x:x+w]
            roi_single = deskew(roi_single)

            side = max(w, h)
            padding = int(side * 0.2)
            container_size = side + padding * 2
            container = np.zeros((container_size, container_size), dtype=np.uint8)
            offset_y = (container_size - h) // 2
            offset_x = (container_size - w) // 2
            
            roi_single = cv2.resize(roi_single, (w, h)) 
            container[offset_y:offset_y+h, offset_x:offset_x+w] = roi_single
            roi_resized = cv2.resize(container, (28, 28), interpolation=cv2.INTER_AREA)
            
            roi_norm = roi_resized.astype('float32') / 255.0
            roi_ready = roi_norm.reshape(28, 28, 1)
            
            batch_rois.append(roi_ready)
            batch_info.append({
                "coords": (rx, ry, w, h),
                "has_hole": item["has_hole"],
                "aspect": item["aspect_ratio"],
                "flat_input": roi_norm.reshape(1, 784) # ç”¨æ–¼ KNN
            })
            
        detected_count = 0
        detected_something = False
        current_frame_text_results = []
        
        # [ä¿®æ”¹] æ–°å¢ä¸€å€‹è¨ˆæ•¸å™¨ï¼Œç”¨æ–¼é¡¯ç¤ºé€£çºŒçš„åºè™Ÿ
        valid_ui_counter = 1

        if len(batch_rois) > 0 and self.model is not None:
            detected_something = True
            try:
                batch_input = np.stack(batch_rois)
                predictions = self.model.predict(batch_input, verbose=0)
                
                for i, pred in enumerate(predictions):
                    top_indices = pred.argsort()[-3:][::-1]
                    res_id = top_indices[0]
                    confidence = pred[res_id]
                    
                    if confidence < CONFIDENCE_THRESHOLD: continue 

                    info = batch_info[i]
                    rx, ry, w, h = info["coords"]
                    has_hole = info["has_hole"]
                    aspect = info["aspect"]
                    
                    # é‚è¼¯åˆ¤æ–·
                    if res_id == 1 and aspect > 0.6: res_id = 7
                    elif res_id == 7 and aspect < 0.25: res_id = 1
                    if res_id == 7 and has_hole: res_id = 9
                    if res_id == 9 and not has_hole and confidence < 0.95: res_id = 7
                    if res_id == 0 and aspect < 0.5: res_id = 1
                    
                    # --- [KNN é›™é‡é©—è­‰] ---
                    final_label_str = str(res_id)
                    box_color = (0, 255, 0) # é è¨­ç¶ è‰²
                    verify_msg = ""
                    
                    if self.knn is not None and KNN_VERIFY_RANGE[0] <= confidence <= KNN_VERIFY_RANGE[1]:
                        try:
                            knn_pred = self.knn.predict(info["flat_input"])[0]
                            if knn_pred != res_id:
                                final_label_str = str(res_id) 
                                verify_msg = f" âš ï¸ KNN: {knn_pred}"
                                box_color = (0, 165, 255) # æ©˜è‰²è¡¨ç¤ºæœ‰ç–‘æ…®
                        except:
                            pass
                    # ----------------------------
                    
                    draw_x = rx + SHRINK_PX
                    draw_y = ry + SHRINK_PX
                    draw_w = max(1, w - (SHRINK_PX * 2))
                    draw_h = max(1, h - (SHRINK_PX * 2))
                    
                    cv2.rectangle(display_img, (draw_x, draw_y), (draw_x+draw_w, draw_y+draw_h), box_color, 2)
                    
                    # [ä¿®æ”¹] ä½¿ç”¨ valid_ui_counter ä¾†é¡¯ç¤ºåºè™Ÿï¼Œè€Œä¸æ˜¯ i+1
                    text_label = f"#{valid_ui_counter}"
                    cv2.putText(display_img, text_label, (rx, ry-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
                    
                    self.cached_rois.append((draw_x, draw_y, draw_w, draw_h, text_label, box_color))
                    
                    # UI æ–‡å­— (Camera æ¨¡å¼ç”¨ç´”æ–‡å­—)
                    # [ä¿®æ”¹] åŒæ­¥ UI æ–‡å­—ä½¿ç”¨æ–°çš„é€£çºŒåºè™Ÿ
                    info_text = f"**#{valid_ui_counter}**: æ•¸å­— `{res_id}` (ä¿¡å¿ƒ: {int(confidence*100)}%){verify_msg}"
                    
                    if confidence < 1.0 and "KNN" not in verify_msg:
                        alt_id = top_indices[1]
                        alt_conf = pred[alt_id]
                        if alt_conf > 0.01:
                            info_text += f" <span style='color:gray'>(æ¬¡é¸: {alt_id})</span>"
                            
                    current_frame_text_results.append(info_text)
                    
                    detected_count += 1
                    valid_ui_counter += 1 # åªæœ‰çœŸæ­£é¡¯ç¤ºæ™‚æ‰ +1
                    
            except Exception as e: 
                print(e)
                pass

        self.detected_count = detected_count
        if detected_something:
             self.ui_results = current_frame_text_results

        # Stability é‚è¼¯ (çœç•¥ç´°ç¯€ä»¥ç¯€çœç‰ˆé¢ï¼Œä¿æŒä¸è®Š)
        if len(raw_boxes_for_stability) == 0:
            self.stability_start_time = None
        elif len(self.last_boxes) == 0:
            self.last_boxes = raw_boxes_for_stability
            self.stability_start_time = time.time()
        else:
            total_movement = 0
            for curr_box in raw_boxes_for_stability:
                c_x, c_y, _, _ = curr_box["box"]
                min_dist = 99999
                for last_box in self.last_boxes:
                    l_x, l_y, _, _ = last_box["box"]
                    dist = abs(c_x - l_x) + abs(c_y - l_y)
                    if dist < min_dist: min_dist = dist
                if min_dist < 30: total_movement += min_dist
                else: total_movement += 20 
            
            count_diff = abs(len(raw_boxes_for_stability) - len(self.last_boxes))
            total_movement += count_diff * 30 
            self.last_boxes = raw_boxes_for_stability

            if total_movement < MOVEMENT_THRESHOLD:
                if self.stability_start_time is None: self.stability_start_time = time.time()
                elapsed = time.time() - self.stability_start_time
                progress = min(elapsed / STABILITY_DURATION, 1.0)
                
                bar_y = h_f - 20 
                bar_w = int(600 * progress)
                color = (0, 255, 255) if progress < 1.0 else (0, 255, 0)
                cv2.rectangle(display_img, (20, bar_y - 15), (20 + bar_w, bar_y), color, -1)
                cv2.rectangle(display_img, (20, bar_y - 15), (w_f - 20, bar_y), (255, 255, 255), 2)
                
                if elapsed >= STABILITY_DURATION and detected_something:
                    self.frozen = True
                    self.frozen_frame = display_img.copy()
            else:
                self.stability_start_time = time.time()

        return av.VideoFrame.from_ndarray(display_img, format="bgr24")

# --- 4. Streamlit ä»‹é¢ ---
st.set_page_config(page_title="æ‰‹å¯«è¾¨è­˜", page_icon="ğŸ“", layout="wide")

if 'stats' not in st.session_state:
    st.session_state['stats'] = {
        'camera': {'total': 0, 'correct': 0}, 
        'handwriting': {'total': 0, 'correct': 0},
        'upload': {'total': 0, 'correct': 0} 
    }
if 'history' not in st.session_state:
    st.session_state['history'] = {'camera': [], 'handwriting': [], 'upload': []} 
    
if 'input_key' not in st.session_state: st.session_state['input_key'] = 0
if 'canvas_key' not in st.session_state: st.session_state['canvas_key'] = "canvas_0"
if 'tracker_state' not in st.session_state: st.session_state['tracker_state'] = {}
if 'next_id' not in st.session_state: st.session_state['next_id'] = 1
    
if 'hw_display_list' not in st.session_state: st.session_state['hw_display_list'] = []
if 'hw_result_img' not in st.session_state: st.session_state['hw_result_img'] = None
if 'hw_result_count' not in st.session_state: st.session_state['hw_result_count'] = 0

if 'upload_display_list' not in st.session_state: st.session_state['upload_display_list'] = []
if 'upload_result_img' not in st.session_state: st.session_state['upload_result_img'] = None
if 'upload_result_count' not in st.session_state: st.session_state['upload_result_count'] = 0
if 'last_uploaded_file_id' not in st.session_state: st.session_state['last_uploaded_file_id'] = None

with st.sidebar:
    st.title("ğŸ›ï¸ æ§åˆ¶å°")
    app_mode = st.radio("æ¨¡å¼é¸æ“‡", ["ğŸ“· æ”å½±æ©Ÿæ¨¡å¼ (Live)", "ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼", "ğŸ“ åœ–ç‰‡ä¸Šå‚³æ¨¡å¼"], index=1)
    
    st.divider()
    
    # --- æˆç¸¾å€å¡Š ---
    st.markdown("### ğŸ“· é¡é ­æˆç¸¾")
    c_total = st.session_state['stats']['camera']['total']
    c_correct = st.session_state['stats']['camera']['correct']
    c_acc = (c_correct / c_total * 100) if c_total > 0 else 0.0
    col_c1, col_c2 = st.columns(2)
    with col_c1: st.metric("ç¸½æ•¸", c_total)
    with col_c2: st.metric("æ­£ç¢º", c_correct)
    st.metric("é¡é ­æº–ç¢ºç‡", f"{c_acc:.1f}%")
    
    col_undo_c, col_reset_c = st.columns(2)
    with col_undo_c:
        if st.button("â†©ï¸ å¾©åŸ", key="undo_cam"):
            if st.session_state['history']['camera']:
                last_entry = st.session_state['history']['camera'].pop()
                st.session_state['stats']['camera']['total'] -= last_entry['total']
                st.session_state['stats']['camera']['correct'] -= last_entry['correct']
                st.rerun()
    with col_reset_c:
        if st.button("ğŸ—‘ï¸ é‡ç½®", key="reset_cam"):
            st.session_state['stats']['camera'] = {'total': 0, 'correct': 0}
            st.session_state['history']['camera'] = []
            st.rerun()

    st.divider()

    st.markdown("### ğŸ¨ æ‰‹å¯«æˆç¸¾")
    h_total = st.session_state['stats']['handwriting']['total']
    h_correct = st.session_state['stats']['handwriting']['correct']
    h_acc = (h_correct / h_total * 100) if h_total > 0 else 0.0
    col_h1, col_h2 = st.columns(2)
    with col_h1: st.metric("ç¸½æ•¸", h_total)
    with col_h2: st.metric("æ­£ç¢º", h_correct)
    st.metric("æ‰‹å¯«æº–ç¢ºç‡", f"{h_acc:.1f}%")

    col_undo_h, col_reset_h = st.columns(2)
    with col_undo_h:
        if st.button("â†©ï¸ å¾©åŸ", key="undo_hw"):
            if st.session_state['history']['handwriting']:
                last_entry = st.session_state['history']['handwriting'].pop()
                st.session_state['stats']['handwriting']['total'] -= last_entry['total']
                st.session_state['stats']['handwriting']['correct'] -= last_entry['correct']
                st.rerun()
    with col_reset_h:
        if st.button("ğŸ—‘ï¸ é‡ç½®", key="reset_hw"):
            st.session_state['stats']['handwriting'] = {'total': 0, 'correct': 0}
            st.session_state['history']['handwriting'] = []
            st.session_state['tracker_state'] = {}
            st.session_state['next_id'] = 1
            st.rerun()

    st.divider()

    st.markdown("### ğŸ“ ä¸Šå‚³æˆç¸¾")
    u_total = st.session_state['stats']['upload']['total']
    u_correct = st.session_state['stats']['upload']['correct']
    u_acc = (u_correct / u_total * 100) if u_total > 0 else 0.0
    col_u1, col_u2 = st.columns(2)
    with col_u1: st.metric("ç¸½æ•¸", u_total)
    with col_u2: st.metric("æ­£ç¢º", u_correct)
    st.metric("ä¸Šå‚³æº–ç¢ºç‡", f"{u_acc:.1f}%")

    col_undo_u, col_reset_u = st.columns(2)
    with col_undo_u:
        if st.button("â†©ï¸ å¾©åŸ", key="undo_up"):
            if st.session_state['history']['upload']:
                last_entry = st.session_state['history']['upload'].pop()
                st.session_state['stats']['upload']['total'] -= last_entry['total']
                st.session_state['stats']['upload']['correct'] -= last_entry['correct']
                st.rerun()
    with col_reset_u:
        if st.button("ğŸ—‘ï¸ é‡ç½®", key="reset_up"):
            st.session_state['stats']['upload'] = {'total': 0, 'correct': 0}
            st.session_state['history']['upload'] = []
            st.session_state['upload_display_list'] = []
            st.session_state['upload_result_img'] = None
            st.session_state['upload_result_count'] = 0
            st.rerun()

st.title("ğŸ“ æ‰‹å¯«æ•¸å­—è¾¨è­˜ç³»çµ±")

with st.expander("ğŸ“– ç³»çµ±æ“ä½œèªªæ˜ (é»æ“Šå±•é–‹)", expanded=False):
    st.markdown(f"""
    #### âš ï¸ æé«˜æº–ç¢ºç‡çš„æŠ€å·§ï¼š
    1. æ‰‹å¯«æ¨¡å¼ä¸­å¦‚æœç™¼ç¾æ²’å‡ºç¾ç¶ è‰²æ¡†ï¼Œä»£è¡¨ä¿¡å¿ƒåº¦éä½æˆ–æ²’åˆ¤å®šåˆ°ï¼Œå¯ä»¥è€ƒæ…®æŠŠå­—å¯«æ•´é½Š
    2. é¡é ­æ¨¡å¼ä¸­è«‹å°‡ç´™å¼µæ‹¿è¿‘é¡é ­ï¼Œæ•¸å­—å¤ªå°æœƒè¢«å¿½ç•¥ï¼Œç­†è·¡å¤ªç´°ä¹Ÿæœƒè¢«å¿½ç•¥ï¼Œç›¡é‡æ‹¿å¥‡ç•°ç­†å¯«ã€‚
    3. æ•¸å­—**1**ä¸è¦ç•«åº•ç·šï¼Œæœƒè¢«åˆ¤å®šæˆå…¶ä»–æ•¸å­—ã€‚
    4. æ•¸å­—ç›¡é‡å¯«æ­£ã€‚
    5. é¡é ­æ¨¡å¼ä¸­é¡¯ç¤ºçš„æ˜¯åºè™Ÿï¼Œå¯¦éš›æ•¸å€¼è«‹é»é¸ğŸ“‹ é¡¯ç¤ºè©³æƒ…æŸ¥çœ‹
    > **æ³¨æ„**ï¼šç³»çµ±è¨­å®šä¿¡å¿ƒåº¦ä½æ–¼ **{int(CONFIDENCE_THRESHOLD*100)}%** çš„çµæœå°‡ä¸æœƒé¡¯ç¤ºã€‚
    """)

if model is None:
    st.error("âŒ æ‰¾ä¸åˆ° `mnist_cnn.h5`ï¼")
    st.stop()

# --- 5. æ¨¡å¼åˆ†æ”¯ ---

if app_mode == "ğŸ“· æ”å½±æ©Ÿæ¨¡å¼ (Live)":
    
    col_cam, col_data = st.columns([2, 1])

    with col_cam:
        ctx = webrtc_streamer(
            key="handwrite-live",
            mode=WebRtcMode.SENDRECV,
            video_processor_factory=HandwriteProcessor,
            media_stream_constraints={"video": {"width": 640, "height": 480}, "audio": False},
            rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
            async_processing=True,
        )

    with col_data:
        st.markdown("### ğŸ“Š è©³ç´°æ•¸æ“š")
        st.caption("è«‹ç­‰å¾…ç•«é¢å‡ºç¾ Captured å¾Œï¼ŒæŒ‰ä¸‹æ–¹æŒ‰éˆ•æ›´æ–°æ•¸æ“š")
        
        col_btn1, col_btn2 = st.columns(2)
        
        with col_btn1:
            if st.button("ğŸ“‹ é¡¯ç¤ºè©³æƒ…", type="secondary", use_container_width=True):
                if ctx.video_processor and ctx.video_processor.frozen:
                    results = ctx.video_processor.ui_results
                    if results:
                        st.success(f"å…±åµæ¸¬åˆ° {len(results)} å€‹æ•¸å­—")
                        st.session_state['last_cam_detected'] = len(results)
                        for line in results:
                            st.markdown(line, unsafe_allow_html=True)
                    else:
                        st.warning("âš ï¸ ç•«é¢å‡çµäº†ï¼Œä½†æ²’æœ‰åµæ¸¬åˆ°æ•¸å­—ã€‚")
                        st.session_state['last_cam_detected'] = 0
                else:
                    st.info("â³ è«‹å…ˆç­‰å¾…é¡é ­ç•«é¢æŠ“æ‹å‡çµ (Captured)...")

        with col_btn2:
            if st.button("ğŸ”„ é‡æ–°æ”å½±", type="primary", use_container_width=True):
                if ctx.video_processor:
                    ctx.video_processor.resume()
                st.session_state['last_cam_detected'] = 0
                st.rerun()

        st.write("---")
        
        manual_score = st.number_input("âœï¸ è¼¸å…¥æ­£ç¢ºæ•¸é‡", min_value=0, value=0, key=f"score_input_{st.session_state['input_key']}")
        
        st.write("##") 
        if st.button("ğŸ’¾ ä¸Šå‚³æˆç¸¾ä¸¦ç¹¼çºŒ", type="primary", use_container_width=True):
            
            total_add = st.session_state.get('last_cam_detected', 0)
            if total_add > 0 and manual_score > total_add:
                st.error(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æ•¸å€¼ ({manual_score}) è¶…éåµæ¸¬ç¸½æ•¸ ({total_add})")
            else:
                if ctx.video_processor:
                    ctx.video_processor.resume()
                
                if total_add == 0: total_add = manual_score

                if manual_score > 0:
                    st.session_state['stats']['camera']['total'] += total_add
                    st.session_state['stats']['camera']['correct'] += manual_score
                    st.session_state['history']['camera'].append({
                        'total': total_add,
                        'correct': manual_score
                    })
                    st.toast(f"âœ… é¡é ­æ¨¡å¼ï¼šå·²è¨˜éŒ„ (ç¸½æ•¸{total_add}/æ­£ç¢º{manual_score})")
                    time.sleep(0.5)
                    st.session_state['input_key'] += 1
                    
                st.rerun()

# ... (å‰é¢çš„ç¨‹å¼ç¢¼ä¿æŒä¸è®Š)

elif app_mode == "ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼":
    
    # [è¼”åŠ©å‡½å¼] é¡¯ç¤ºä¿¡å¿ƒåº¦æ¢
    def get_bar_html(confidence, is_uncertain=False):
        percent = min(int(confidence * 100), 100)
        if is_uncertain: color = "#ff9f43" 
        elif confidence > 0.95: color = "#2ecc71"
        elif confidence > 0.85: color = "#f1c40f"
        else: color = "#e74c3c"
        
        return f"""
        <div style="display: flex; align-items: center; margin-top: 4px;">
            <div style="width: 50%; height: 8px; background-color: #444; border-radius: 4px; overflow: hidden;">
                <div style="width: {percent}%; height: 100%; background-color: {color};"></div>
            </div>
            <span style="margin-left: 8px; font-size: 0.8em; color: {color};">{percent}%</span>
        </div>
        """

    # --- ç‰ˆé¢é…ç½® ---
    # ... (åœ¨ elif app_mode == "ğŸ¨ æ‰‹å¯«æ¿æ¨¡å¼": è£¡é¢) ...

    c_left, c_right = st.columns([3, 2])
    
    with c_right:
        st.markdown("### ğŸ‘ï¸ çµæœ")
        result_image_placeholder = st.empty()
        
        # é¡¯ç¤ºåœ–ç‰‡çš„é‚è¼¯ (ä¹‹å‰æ”¹éçš„é»‘è‰²ç©ºåœ–é‚è¼¯)
        if st.session_state['hw_result_img'] is not None:
             result_image_placeholder.image(st.session_state['hw_result_img'], channels="BGR", use_container_width=True)
        else:
             blank_img = np.zeros((400, 600, 3), dtype=np.uint8)
             cv2.putText(blank_img, "Waiting...", (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 100, 100), 2)
             result_image_placeholder.image(blank_img, channels="BGR", use_container_width=True, caption="è«‹åœ¨å·¦å´æ›¸å¯«")

        st.write("---")
        
        # â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
        # [é‡è¦] é€™è£¡ä¸€å®šè¦å»ºç«‹é€™å€‹ä½”ä½ç¬¦ï¼Œè¼¸å…¥æ¡†æ‰æœƒå‡ºç¾åœ¨é€™è£¡ï¼
        result_stats_placeholder = st.empty()
        # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

    current_results_list = []
    
    # 2. å·¦å´ç•«å¸ƒèˆ‡é‚è¼¯
    with c_left:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤ç•«å¸ƒ"):
            st.session_state['canvas_key'] = f"canvas_{time.time()}"
            st.session_state['tracker_state'] = {}
            st.session_state['next_id'] = 1
            st.session_state['hw_display_list'] = [] 
            st.session_state['hw_result_img'] = None
            st.session_state['hw_result_count'] = 0
            st.rerun()

        canvas_result = st_canvas(
            fill_color="rgba(255, 165, 0, 0.3)",
            stroke_width=15,
            stroke_color="#FFFFFF",
            background_color="#000000",
            height=400,  
            width=850,   
            drawing_mode="freedraw",
            key=st.session_state['canvas_key'],
            display_toolbar=False,
            update_streamlit=True, 
        )
        
        # --- æ ¸å¿ƒè™•ç†é‚è¼¯ (ä¿æŒåŸæœ¬é‚è¼¯ä¸è®Š) ---
        if canvas_result.image_data is not None:
            img_data = canvas_result.image_data.astype(np.uint8)
            
            if np.max(img_data) > 0:
                if img_data.shape[2] == 4:
                    img_data = cv2.cvtColor(img_data, cv2.COLOR_RGBA2BGR)
                gray = cv2.cvtColor(img_data, cv2.COLOR_BGR2GRAY)

                binary_proc = cv2.dilate(gray, None, iterations=1)
                _, binary_proc = cv2.threshold(binary_proc, 127, 255, cv2.THRESH_BINARY)
                contours, _ = cv2.findContours(binary_proc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                tracked_items = update_tracker(contours)
                
                draw_img = img_data.copy()
                batch_rois = []
                flat_inputs = [] 
                
                for item in tracked_items:
                    cnt = item['cnt']
                    x, y, w, h = cv2.boundingRect(cnt)
                    
                    if cv2.contourArea(cnt) > MIN_AREA:
                        roi = binary_proc[y:y+h, x:x+w]
                        roi = deskew(roi) 
                        
                        side = max(w, h)
                        pad = int(side * 0.2)
                        container = np.zeros((side+pad*2, side+pad*2), dtype=np.uint8)
                        ox, oy = (side+pad*2-w)//2, (side+pad*2-h)//2
                        
                        roi = cv2.resize(roi, (w, h))
                        container[oy:oy+h, ox:ox+w] = roi
                        
                        roi_ready = cv2.resize(container, (28, 28), interpolation=cv2.INTER_AREA)
                        roi_norm = roi_ready.astype('float32') / 255.0
                        
                        batch_rois.append(roi_norm.reshape(28, 28, 1))
                        flat_inputs.append(roi_norm.reshape(1, 784))
                
                detected_count = 0
                valid_ui_counter = 1

                if len(batch_rois) > 0:
                    preds = model.predict(np.stack(batch_rois), verbose=0)
                    
                    for i, pred in enumerate(preds):
                        item = tracked_items[i]
                        cnt = item['cnt']
                        
                        top_indices = pred.argsort()[-3:][::-1]
                        res_id = top_indices[0]
                        confidence = pred[res_id]
                        
                        if confidence < CONFIDENCE_THRESHOLD:
                            continue

                        x, y, w, h = cv2.boundingRect(cnt)
                        asp = w/h
                        
                        if res_id==1 and asp>0.6: res_id=7 
                        if res_id==7 and asp<0.3: res_id=1
                        
                        is_uncertain = False
                        verify_text_html = ""
                        final_res = str(res_id)
                        box_color = (0, 255, 0)
                        
                        if knn_model is not None and KNN_VERIFY_RANGE[0] <= confidence <= KNN_VERIFY_RANGE[1]:
                            try:
                                k_pred = knn_model.predict(flat_inputs[i])[0]
                                if k_pred != res_id:
                                    is_uncertain = True
                                    verify_text_html = f"<div style='color:#ff9f43; font-size:0.85em; margin-bottom: 2px;'>âš ï¸ KNN å»ºè­°: {k_pred}</div>"
                                    final_res = str(res_id)
                                    box_color = (0, 165, 255)
                            except: pass
                        
                        cv2.rectangle(draw_img, (x, y), (x+w, y+h), box_color, 2)
                        cv2.putText(draw_img, final_res, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
                        
                        text_part = f"<div>#{valid_ui_counter}: æ•¸å­— <strong>{res_id}</strong></div>"
                        if is_uncertain: text_part += verify_text_html
                        elif confidence < 1.0: 
                            alts = []
                            for alt_idx in top_indices[1:]:
                                if pred[alt_idx] > 0.01: alts.append(f"{alt_idx}({int(pred[alt_idx]*100)}%)")
                            if alts: text_part += f"<div style='color:gray; font-size:0.8em'>âš ï¸ å…¶ä»–: {', '.join(alts)}</div>"
                        
                        bar_part = get_bar_html(confidence, is_uncertain)
                        current_results_list.append(f"<div style='margin-bottom:10px;'>{text_part}{bar_part}</div>")
                        
                        detected_count += 1
                        valid_ui_counter += 1

                # æ›´æ–°åœ–ç‰‡èˆ‡ç‹€æ…‹
                result_image_placeholder.image(draw_img, channels="BGR", use_container_width=True)
                
                st.session_state['hw_display_list'] = current_results_list
                st.session_state['hw_result_img'] = draw_img
                st.session_state['hw_result_count'] = detected_count

    # 3. é¡¯ç¤ºä¸‹æ–¹çš„è©³ç´°æ•¸æ“š
    with c_left:
        if st.session_state['hw_display_list']:
            st.write("---")
            st.markdown("#### ğŸ“Š è©³ç´°æ•¸æ“š:")
            cols = st.columns(2)
            for i, html_content in enumerate(st.session_state['hw_display_list']):
                cols[i % 2].markdown(html_content, unsafe_allow_html=True)

    status_placeholder = st.empty()
    
    final_count = st.session_state['hw_result_count']
    
    wrapper_style = "min-height: 60px; margin-bottom: 10px;"
    
    if final_count > 0:
        # ç¶ è‰²ç‹€æ…‹
        status_html = f"""
        <div style="{wrapper_style}">
            <div style="
                padding: 10px;
                border-radius: 5px;
                background-color: #d1e7dd; 
                color: #0f5132;
                border: 1px solid #badbcc;">
                âœ… åµæ¸¬åˆ°: <strong>{final_count}</strong> å€‹
            </div>
        </div>
        """
    else:
        # è—è‰²ç‹€æ…‹ (ä½”ä½)
        status_html = f"""
        <div style="{wrapper_style}">
            <div style="
                padding: 10px;
                border-radius: 5px;
                background-color: #cff4fc;
                color: #055160;
                border: 1px solid #b6effb;">
                â„¹ï¸ ç­‰å¾…æ›¸å¯«ä¸­...
            </div>
        </div>
        """
        
    status_placeholder.markdown(status_html, unsafe_allow_html=True)
    # â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²
    
    # [é—œéµ] è¼¸å…¥æ¡†æ”¾åœ¨ placeholder å¤–é¢ï¼
    # é€™æ¨£ status_placeholder æ›´æ–°æ™‚ï¼Œé€™å€‹è¼¸å…¥æ¡†å°±ä¸æœƒè¢«éŠ·æ¯€é‡è“‹
    hw_score = st.number_input("è¼¸å…¥æ•¸é‡", min_value=0, value=final_count, key="hw_input")
    
    st.write("##")
    if st.button("ğŸ’¾ ä¸Šå‚³æˆç¸¾", key="hw_save", type="primary"):
        if hw_score > final_count:
            st.error(f"âŒ éŒ¯èª¤ï¼šè¼¸å…¥æ•¸å€¼ ({hw_score}) è¶…éåµæ¸¬ç¸½æ•¸ ({final_count})")
        else:
            st.session_state['stats']['handwriting']['total'] += final_count
            st.session_state['stats']['handwriting']['correct'] += hw_score
            st.session_state['history']['handwriting'].append({'total': final_count, 'correct': hw_score})
            
            # é‡ç½®ç‹€æ…‹
            st.session_state['canvas_key'] = f"canvas_{time.time()}"
            st.session_state['tracker_state'] = {}
            st.session_state['next_id'] = 1
            st.session_state['hw_display_list'] = []
            st.session_state['hw_result_img'] = None
            st.session_state['hw_result_count'] = 0
            
            if 'hw_input' in st.session_state:
                del st.session_state['hw_input']
            
            st.toast("âœ… æ‰‹å¯«æˆç¸¾å·²å„²å­˜ï¼")
            time.sleep(0.5)
            st.rerun()

elif app_mode == "ğŸ“ åœ–ç‰‡ä¸Šå‚³æ¨¡å¼":

    # --- 1. ä¾†æºåˆ¤æ–· ---
    def detect_image_source(img_bgr):
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        return "digital" if (np.sum(gray > 250) / gray.size) > 0.3 else "photo"

    # --- 2. ç‰©ç†èåˆ ---
    def merge_overlapping_boxes(boxes):
        if len(boxes) < 2: return boxes
        merged = []
        while len(boxes) > 0:
            curr = boxes.pop(0)
            x1, y1, w1, h1 = curr
            rx1, ry1 = x1 + w1, y1 + h1
            has_overlap = False
            i = 0
            while i < len(boxes):
                next_box = boxes[i]
                x2, y2, w2, h2 = next_box
                rx2, ry2 = x2 + w2, y2 + h2
                pad = 15
                overlap = not ((rx1 + pad) < x2 or (x1 - pad) > rx2 or (ry1 + pad) < y2 or (y1 - pad) > ry2)
                if overlap:
                    new_x = min(x1, x2)
                    new_y = min(y1, y2)
                    new_w = max(rx1, rx2) - new_x
                    new_h = max(ry1, ry2) - new_y
                    curr = (new_x, new_y, new_w, new_h)
                    x1, y1, w1, h1 = curr
                    rx1, ry1 = new_x + new_w, new_y + new_h
                    boxes.pop(i)
                    has_overlap = True
                else:
                    i += 1
            if has_overlap:
                boxes.insert(0, curr)
            else:
                merged.append(curr)
        return merged

    # --- 3. [åˆ†æµä¿®æ­£] å°ºå¯¸éæ¿¾å™¨ (æ•¸ä½å¯¬é¬†ï¼Œæ‰‹æ©Ÿåš´æ ¼) ---
    def filter_small_boxes(boxes, img_height, source_type):
        if not boxes: return []
        
        # 1. æ•¸ä½æ¨¡å¼ï¼šæ¥µåº¦å¯¬é¬† (ä¿è­· 2)
        if source_type == "digital":
            kept = []
            for box in boxes:
                # åªè¦ä¸æ˜¯å¥ˆç±³ç´šé›œé» (h>15) å°±ä¿ç•™
                if box[3] > 15: kept.append(box)
            return kept

        # 2. æ‰‹æ©Ÿæ¨¡å¼ï¼šåš´æ ¼éæ¿¾ (æ®ºæ±¡æ¼¬)
        
        # çµ•å°åº•ç·š (2%)
        abs_min_h = int(img_height * 0.02)
        
        # è¨ˆç®—ä¸­ä½æ•¸ (åªç”¨æœ‰æ•ˆæ¡†)
        valid_h = [b[3] for b in boxes if b[3] > abs_min_h]
        valid_area = [b[2]*b[3] for b in boxes if b[3] > abs_min_h]
        
        median_h = np.median(valid_h) if valid_h else 0
        median_area = np.median(valid_area) if valid_area else 0
        
        kept_boxes = []
        for box in boxes:
            w, h = box[2], box[3]
            area = w * h
            aspect = w / float(h)
            
            # [è¦å‰‡ A] çµ•å°åº•ç·š
            if h < abs_min_h: continue
            
            # [è¦å‰‡ B] ç˜¦å­ä¿è­· (é‡å° 1)
            if aspect < 0.35:
                # ç˜¦å­åªè¦æœ‰ 35% å¹³å‡èº«é«˜å°±é
                if median_h > 0 and h > (median_h * 0.35):
                    kept_boxes.append(box)
                continue
            
            # [è¦å‰‡ C] ä¸€èˆ¬ç‰©ä»¶ (é‡å° 0, 2, æ±¡æ¼¬)
            # 1. èº«é«˜å¿…é ˆé”åˆ°ä¸­ä½æ•¸çš„ 50%
            if median_h > 0 and h < (median_h * 0.5):
                continue
                
            # 2. é¢ç©å¿…é ˆé”åˆ°ä¸­ä½æ•¸çš„ 20% (æ®ºå°åœ“é»)
            if median_area > 0 and area < (median_area * 0.2):
                continue

            kept_boxes.append(box)
            
        return kept_boxes

    # --- 4. å¢¨æ°´æ¿ƒåº¦éæ¿¾ ---
    def filter_low_contrast_boxes(boxes, gray_img):
        if not boxes: return []
        flat = np.sort(gray_img.ravel())
        ink_black = np.mean(flat[:int(len(flat)*0.02)])
        paper_bg = np.median(flat)
        dynamic_range = paper_bg - ink_black
        threshold = paper_bg - (dynamic_range * 0.6)
        
        kept_boxes = []
        for box in boxes:
            x, y, w, h = box
            roi = gray_img[y:y+h, x:x+w]
            if roi.size == 0: continue
            roi_flat = np.sort(roi.ravel())
            roi_darkest = np.mean(roi_flat[:max(1, int(len(roi_flat)*0.1))])
            if roi_darkest > threshold: continue
            kept_boxes.append(box)
        return kept_boxes

    # --- 5. MNIST æ¨™æº–åŒ– ---
    def preprocess_for_mnist(roi_binary):
        h, w = roi_binary.shape
        canvas = np.zeros((28, 28), dtype=np.uint8)
        scale = 20.0 / max(h, w)
        nh = max(1, int(h * scale))
        nw = max(1, int(w * scale))
        roi_resized = cv2.resize(roi_binary, (nw, nh), interpolation=cv2.INTER_AREA)
        y_off = (28 - nh) // 2
        x_off = (28 - nw) // 2
        y_end = min(y_off + nh, 28)
        x_end = min(x_off + nw, 28)
        canvas[y_off:y_end, x_off:x_end] = roi_resized[:y_end-y_off, :x_end-x_off]
        
        _, canvas = cv2.threshold(canvas, 10, 255, cv2.THRESH_BINARY)
        
        M = cv2.moments(canvas)
        if M["m00"] > 0:
            cx = M["m10"] / M["m00"]
            cy = M["m01"] / M["m00"]
            shift_x = 14 - cx
            shift_y = 14 - cy
            M_shift = np.float32([[1, 0, shift_x], [0, 1, shift_y]])
            canvas = cv2.warpAffine(canvas, M_shift, (28, 28))
        canvas = cv2.dilate(canvas, None, iterations=1)
        return canvas

    # --- ä¿¡å¿ƒåº¦æ¢ ---
    def get_bar_html(confidence, is_uncertain=False):
        percent = min(int(confidence * 100), 100)
        color = "#e74c3c"
        if is_uncertain: color = "#ff9f43"
        elif confidence > 0.95: color = "#2ecc71"
        elif confidence > 0.85: color = "#f1c40f"
        return f"""
        <div style="display: flex; align-items: center; margin-top: 4px;">
            <div style="width: 50%; height: 8px; background-color: #444; border-radius: 4px; overflow: hidden;">
                <div style="width: {percent}%; height: 100%; background-color: {color};"></div>
            </div>
            <span style="margin-left: 8px; font-size: 0.8em; color: {color};">{percent}%</span>
        </div>
        """

    col_up_left, col_up_right = st.columns([3, 1])
    
    with col_up_left:
        uploaded_file = st.file_uploader("è«‹ä¸Šå‚³åœ–ç‰‡ (JPG, PNG)", type=['png', 'jpg', 'jpeg'])
        
        if uploaded_file is not None:
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            img = cv2.imdecode(file_bytes, 1)
            
            if st.session_state['last_uploaded_file_id'] != uploaded_file.file_id:
                st.session_state['last_uploaded_file_id'] = uploaded_file.file_id
                
                source_type = detect_image_source(img)
                display_img = img.copy()
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                
                # å‰è™•ç†
                if source_type == "photo":
                    st.info("ğŸ“¸ æ¨¡å¼ï¼šæ‰‹æ©Ÿç¿»æ‹ (åš´æ ¼é™¤å¢)")
                    filtered = cv2.bilateralFilter(gray, 9, 75, 75)
                    thresh = cv2.adaptiveThreshold(filtered, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                                   cv2.THRESH_BINARY_INV, 45, 12)
                    kernel_connect = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
                    binary_proc = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel_connect)
                    min_area_limit = 10 
                else:
                    st.success("ğŸ’» æ¨¡å¼ï¼šæ•¸ä½æˆªåœ–")
                    _, binary_proc = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
                    kernel_connect = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
                    binary_proc = cv2.morphologyEx(binary_proc, cv2.MORPH_CLOSE, kernel_connect)
                    min_area_limit = 30

                with st.expander("ğŸ‘€ Debug: æ©Ÿå™¨çœ‹åˆ°çš„ç•«é¢"):
                    st.image(binary_proc, caption="äºŒå€¼åŒ–çµæœ", clamp=True, channels='GRAY')

                contours, hierarchy = cv2.findContours(binary_proc, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                raw_boxes = []
                for cnt in contours:
                    area = cv2.contourArea(cnt)
                    x, y, w, h = cv2.boundingRect(cnt)
                    if area < min_area_limit: continue
                    if h < 5: continue 
                    raw_boxes.append((x, y, w, h))

                merged_boxes = merge_overlapping_boxes(raw_boxes)
                
                # [é—œéµ] åˆ†æµéæ¿¾ (å‚³å…¥ source_type)
                h_img_total = img.shape[0]
                sized_boxes = filter_small_boxes(merged_boxes, h_img_total, source_type)
                
                # åªæœ‰ç…§ç‰‡æ‰éœ€è¦å¢¨æ°´éæ¿¾
                final_boxes = sized_boxes
                if source_type == "photo":
                    final_boxes = filter_low_contrast_boxes(sized_boxes, gray)

                batch_rois = []
                batch_info = []
                
                for (x, y, w, h) in final_boxes:
                    roi = binary_proc[y:y+h, x:x+w]
                    
                    if source_type == "photo" and h < 150:
                        roi = deskew(roi)
                    
                    final_norm = preprocess_for_mnist(roi)
                    final_input = final_norm.astype('float32') / 255.0
                    
                    has_hole = False
                    roi_u8 = (final_input * 255).astype(np.uint8)
                    _, t_roi = cv2.threshold(roi_u8, 50, 255, cv2.THRESH_BINARY)
                    c_sub, h_sub = cv2.findContours(t_roi, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)
                    if h_sub is not None:
                        for idx, cc in enumerate(c_sub):
                            if h_sub[0][idx][3] != -1 and cv2.contourArea(cc) > 5:
                                has_hole = True
                                break

                    batch_rois.append(final_input.reshape(28, 28, 1))
                    batch_info.append({
                        "rect": (x, y, w, h),
                        "has_hole": has_hole,
                        "aspect": w / float(h),
                        "flat_input": final_input.reshape(1, 784)
                    })

                detected_count = 0
                results_text = []
                valid_ui_counter = 1
                
                h_disp, w_disp = display_img.shape[:2]
                scale = max(1.0, w_disp / 800.0)
                font_s = 1.0 * scale
                thick = max(2, int(3 * scale))

                if len(batch_rois) > 0:
                    combined = list(zip(batch_rois, batch_info))
                    combined.sort(key=lambda x: x[1]["rect"][0])
                    
                    sorted_rois = [x[0] for x in combined]
                    sorted_info = [x[1] for x in combined]
                    
                    predictions = model.predict(np.stack(sorted_rois), verbose=0)
                    
                    for i, pred in enumerate(predictions):
                        top_indices = pred.argsort()[-3:][::-1]
                        res_id = top_indices[0]
                        confidence = pred[res_id]
                        
                        info = sorted_info[i]
                        x, y, w, h = info["rect"]
                        has_hole = info["has_hole"]
                        aspect = info["aspect"]

                        thresh = CONFIDENCE_THRESHOLD
                        if h > 150: thresh = 0.5
                        if confidence < thresh: continue

                        if res_id == 7 and aspect < 0.25: res_id = 1
                        if res_id == 1 and has_hole: res_id = 0
                        if source_type == "digital" and aspect < 0.2: res_id = 1
                        
                        color = (0, 255, 0)
                        extra_msg = ""
                        
                        if knn_model is not None and KNN_VERIFY_RANGE[0] <= confidence <= 0.99:
                             try:
                                k_res = knn_model.predict(info["flat_input"])[0]
                                if k_res != res_id:
                                    extra_msg = f" (KNN: {k_res})"
                                    if res_id == 8 and k_res == 9 and has_hole:
                                        res_id = 9
                                        color = (0, 165, 255)
                             except: pass

                        cv2.rectangle(display_img, (x, y), (x+w, y+h), color, thick)
                        cv2.putText(display_img, str(res_id), (x, y - 5), 
                                    cv2.FONT_HERSHEY_SIMPLEX, font_s, (0, 0, 255), thick)

                        text_html = f"<div><strong>#{valid_ui_counter}</strong>: {res_id} <span style='font-size:0.8em; color:gray'>{extra_msg}</span></div>"
                        bar_html = get_bar_html(confidence)
                        results_text.append(f"<div style='margin-bottom:8px'>{text_html}{bar_html}</div>")
                        
                        detected_count += 1
                        valid_ui_counter += 1

                st.session_state['upload_result_img'] = display_img
                st.session_state['upload_display_list'] = results_text
                st.session_state['upload_result_count'] = detected_count

            if st.session_state['upload_result_img'] is not None:
                st.image(st.session_state['upload_result_img'], channels="BGR", use_container_width=True)
            
            if st.session_state['upload_display_list']:
                st.divider()
                st.markdown("#### ğŸ“Š è¾¨è­˜çµæœ")
                cols = st.columns(3)
                for idx, txt in enumerate(st.session_state['upload_display_list']):
                    cols[idx % 3].markdown(txt, unsafe_allow_html=True)

    with col_up_right:
        st.markdown("### ğŸ“ ç¢ºèª")
        final_cnt = st.session_state['upload_result_count']
        
        if final_cnt > 0:
            st.success(f"åµæ¸¬åˆ° {final_cnt} å€‹")
        else:
            if uploaded_file: st.warning("æœªåµæ¸¬åˆ°")
            
        real_val = st.number_input("æ­£ç¢ºæ•¸é‡", min_value=0, value=final_cnt, key="up_input_val")
        
        st.write("##")
        if st.button("ğŸ’¾ å„²å­˜", type="primary", use_container_width=True):
            if final_cnt == 0 and real_val == 0:
                st.toast("ç„¡è³‡æ–™")
            else:
                save_val = final_cnt if final_cnt > 0 else real_val
                st.session_state['stats']['upload']['total'] += save_val
                st.session_state['stats']['upload']['correct'] += real_val
                st.session_state['history']['upload'].append({'total': save_val, 'correct': real_val})
                st.toast("âœ… å·²å„²å­˜")
                st.session_state['upload_result_img'] = None
                st.session_state['upload_display_list'] = []
                st.session_state['upload_result_count'] = 0
                st.session_state['last_uploaded_file_id'] = None
                time.sleep(0.5)
                st.rerun()